{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vcf\n",
    "import gzip\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "#from jupyterthemes  import jtplot\n",
    "\n",
    "#jtplot.style(theme=\"onedork\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_set(rareness,clin,pato,rev,columns,homo=None):\n",
    "    '''\n",
    "    rareness= binario si tomar el subset con AF segun definimos anteriormente\n",
    "    homo=vector de homozigosidad minima y maxima para filtrar\n",
    "    pato= vector con tipos de patogeneidad a seleccionar\n",
    "    rev= status de review permitidos de la clasificacion\n",
    "    clin= si usar clinvar para definir categoria\n",
    "                 '''\n",
    "    if clin:\n",
    "    \n",
    "        df_cln_info_pat=pd.read_csv(\"df_cln_info_pat\")\n",
    "        df_cln_info_pat['patho_rev'] = df_cln_info_pat['patho_rev'].apply(eval)\n",
    "        \n",
    "\n",
    "        df=df_cln_info_pat.loc[df_cln_info_pat['patho_status'].isin(pato)]\n",
    "        \n",
    "        #print(df.head)\n",
    "\n",
    "\n",
    "        #df=df.loc[df['patho_rev'].isin(rev)]\n",
    "        df=df.loc[df.patho_rev.apply(lambda x:x in rev).values]\n",
    "\n",
    "        #print(df.head)\n",
    "    if rareness:\n",
    "        variants_dbfnsp=pd.read_csv(\"~/rare_control_variants_pops_dbNSFP4.0b2a.txt.tsv\",sep=\"\\t\")\n",
    "        variants_dbfnsp=variants_dbfnsp[[\"clinvar_id\",\"Ensembl_geneid\"]+columns+[\"gnomAD_exomes_controls_nhomalt\"]]\n",
    "        \n",
    "        if homo != None:\n",
    "        \n",
    "            variants_dbfnsp=variants_dbfnsp.loc[(variants_dbfnsp[\"gnomAD_exomes_controls_nhomalt\"]>homo[0])&(variants_dbfnsp[\"gnomAD_exomes_controls_nhomalt\"]<homo[1])]\n",
    "    else:\n",
    "       \n",
    "        reader = pd.read_csv('/home/felipe/dbNSFP4.0b2a.txt.gz',sep=\"\\t\", chunksize=100000,low_memory=False,na_values = '.',usecols=[\"clinvar_id\",\"Ensembl_geneid\"]+columns)\n",
    "    \n",
    "        lchunk=[] \n",
    "        for k,chunk in enumerate(reader):\n",
    "            \n",
    "            ii = chunk[\"clinvar_id\"].isin(df[\"clinvar_id\"])\n",
    "            if sum(ii)==0:\n",
    "                continue\n",
    "            \n",
    "            chunkf = chunk.loc[ii] \n",
    "           # print(chunkf.head)\n",
    "\n",
    "            #chunkf = chunkf[[\"clinvar_id\"]+columns]\n",
    "\n",
    "            lchunk.append(chunkf)\n",
    "\n",
    "        variants_dbfnsp = pd.concat(lchunk)\n",
    "\n",
    "   \n",
    "    if clin:\n",
    "            \n",
    "            df=df[['clinvar_id','auto_dominant','auto_recessive', 'x_recessive', 'x_dominant']]\n",
    "        \n",
    "            variants_dbfnsp=variants_dbfnsp.merge(df,how=\"right\",on=\"clinvar_id\",)\n",
    "    \n",
    "    return(variants_dbfnsp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria used for rare variants:\n",
    "# those variants \n",
    "def extract_rareVariants_from_dbNSFP(in_f, out_f, size, freqcols , max_frequency_threshold):\n",
    "    reader = pd.read_table(in_f, chunksize=size,na_values = '.')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for k,chunk in enumerate(reader):\n",
    "        minutes = (time.time()-t0)/float(60)\n",
    "        if k&(k-1)==0:\n",
    "            print('processed %s*%s rows. Elapsed, %s minutes',(k+1,size,minutes))\n",
    "        ii = (chunk[freqcols].isna()).sum(axis=1)>0\n",
    "        chunkf = chunk[ii] \n",
    "        max_freq_observed = chunkf[freqcols].max(axis=1)\n",
    "        chunkf['max_freq_observed'] =  max_freq_observed\n",
    "        \n",
    "        # keep variants with max_freq_observed under threshold  \n",
    "        # ALSO exclude those with maximum freq = 0.0000 (only for sanity)\n",
    "        flag_rare_variant =  (max_freq_observed < max_frequency_threshold)&(max_freq_observed >0 )\n",
    "        rares = chunkf[flag_rare_variant]\n",
    "\n",
    "        if k==0:\n",
    "            rares.to_csv(out_f, index=False,sep='\\t') \n",
    "        else:\n",
    "            rares.to_csv(out_f, index=False, header=False, mode='a',sep='\\t')\n",
    "        gc.collect()\n",
    "    return(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_proteinwise_scores(word,fun=max):\n",
    "    if pd.isna(word):\n",
    "        return(None)\n",
    "    if not (type(word) is str):\n",
    "        return(word)\n",
    "        \n",
    "    averga=re.split(\";\",word)\n",
    "    averga=set(averga)\n",
    "    averga.discard(\";\")\n",
    "    averga.discard(\".\")\n",
    "    averga.discard(\",\")\n",
    "    averga.discard(\"\")\n",
    "    \n",
    "    if averga==set():\n",
    "        return(None)\n",
    "    \n",
    "    mara=map(float,averga)\n",
    "\n",
    "    av=list(mara)\n",
    "\n",
    "    return(max(av))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens(obj):\n",
    "    #aver=re.split(r';',obj)\n",
    "    #aver=aver[0]\n",
    "    obj=str(obj)\n",
    "    \n",
    "    if not (';' in obj):\n",
    "        return(obj)\n",
    "    aver=re.search(r'(.*?);', obj).group(1)\n",
    "    #k=\"\"\n",
    "    #for i in aver:\n",
    "     #   k=k+i\n",
    "    #return(k)\n",
    "    return(aver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forge_graph_features(df_rare_pat,target=\"target\"):\n",
    "    import igraph\n",
    "\n",
    "\n",
    "\n",
    "    biogrid=pd.read_csv(\"~/BIOGRID/BIOGRID-ORGANISM-3.5.168.tab2/BIOGRID-ORGANISM-Homo_sapiens-3.5.168.tab2.txt\",low_memory=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "    df_biogrid=biogrid[['Entrez Gene Interactor A','Entrez Gene Interactor B']]\n",
    "\n",
    "    tuples = [tuple(x) for x in df_biogrid.values]\n",
    "    \n",
    "    Gm = igraph.Graph.TupleList(tuples, directed = True, edge_attrs = ['weight'])\n",
    "\n",
    "\n",
    "    dbfsnp_gen= pd.read_csv('~/dbNSFP4.0b2_gene.gz', compression='gzip',sep=\"\\t\",low_memory=False)\n",
    "\n",
    "\n",
    "    dbfsnp_gen.rename(columns={\"Ensembl_gene\":\"Ensembl_geneid\"},inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_ensem_to_entres=dbfsnp_gen[[\"Ensembl_geneid\",\"Entrez_gene_id\"]]\n",
    "################################################################################\n",
    "    df_rare_pat=df_rare_pat.merge(df_ensem_to_entres,how=\"left\",on=[\"Ensembl_geneid\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Gm.vs[\"pathogenicity\"]=[id in df_rare_pat.loc[ df_rare_pat[target]>0][\"Entrez_gene_id\"] for id in Gm.vs[\"name\"]   ]\n",
    "\n",
    "\n",
    "    v_patho=[vertex[\"name\"] for vertex in Gm.vs if vertex[\"pathogenicity\"]]\n",
    "\n",
    "\n",
    "    def get_min_dist_to_patho(Vtx):\n",
    "        path_to_patho=[len(path)for path in Vtx.get_shortest_paths(v_patho)]\n",
    "        len_to_patho=list(set(path_to_patho))\n",
    "\n",
    "        len_to_patho.sort()\n",
    "        if(len(len_to_patho)==1):\n",
    "            return(-1000)\n",
    "\n",
    "        return(len_to_patho[1])\n",
    "\n",
    "\n",
    "\n",
    "    Gm.vs[\"dist_to_patho\"]=[ get_min_dist_to_patho(Vtx)  for Vtx in Gm.vs]\n",
    "\n",
    "   \n",
    "    def get_number_patho_first_neigh(Vtx):\n",
    "        if(len(Gm.neighbors(Vtx))):\n",
    "           return(0)\n",
    "        sum(Gm.vs[list(set(Gm.neighbors(Vtx)))][\"pathogenicity\"])\n",
    "\n",
    "\n",
    "    Gm.vs[\"patho_first_neigh\"]=[ get_number_patho_first_neigh(Vtx)  for Vtx in Gm.vs]\n",
    "\n",
    "\n",
    "    Gm.save(\"bio_grid_graph\",format=\"picklez\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    susu=pd.DataFrame( {\"Entrez_gene_id\":Gm.vs[\"name\"] , \"net_dis\":Gm.vs[\"dist_to_patho\"],\"net_nn\":Gm.vs[\"patho_first_neigh\"]})\n",
    "    \n",
    "    df_rare_pat.Entrez_gene_id.loc[df_rare_pat.Entrez_gene_id.isna()]=-2\n",
    "    \n",
    "    susu=df_rare_pat.merge(susu,how=\"left\",on=[\"Entrez_gene_id\"])\n",
    "    return(susu)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cln_info_pat=pd.read_csv(\"df_cln_info_pat\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
